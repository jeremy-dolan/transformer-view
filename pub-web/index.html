<!DOCTYPE html>

<html lang="en">
<head prefix="og: http://ogp.me/ns#">
	<meta charset="utf-8">

	<!-- Tell clients it's ok to use the device's natively-optimized viewport. Without this, mobile platforms use some
	desktop-ish width (often 980px) for rendering, and then scale the result to fit the screen. (This is to handle old
	web pages which assume clients have a desktop-sized display.) -->
	<meta name="viewport" content="width=device-width">

	<!-- Open Graph properties -->
	<!-- Used for link previews by Facebook/Twitter/LinkedIn/iMessage/Reddit/Slack/Discord/etc. -->
	<meta property="og:type" content="website">
	<meta property="og:title" content="A Vector-Level View of GPT-2">
	<meta property="og:image" content="https://jeremydolan.net/transformer-view/preview.png">
	<meta property="og:image:width" content="1200">
	<meta property="og:image:height" content="630">
	<meta property="og:url" content="https://jeremydolan.net/transformer-view/">
	<meta property="og:description" content="Computational infographic of vector-wise inference in a decoder-only transformer">

	<!-- Icons -->
	<!-- The icon is pixel art so nearest-neighbor upscaling would be fine--but some clients interpolate, and some use
	the 16x16 icon with 16px of padding instead of upscaling. So we need a 32x32 version, and maybe also 48x48. -->
	<link rel="icon" sizes="16x16" type="image/png" href="/media/icons/16.png">
	<link rel="icon" sizes="32x32" type="image/png" href="/media/icons/32.png">
	<link rel="icon" sizes="48x48" type="image/png" href="/media/icons/48.png">
	<link rel="apple-touch-icon" sizes="180x180" type="image/png" href="/media/icons/180.png">
	<!-- NB: There is also an (undeclared) /favicon.ico Microsoft x-icon file containing all three (16, 32, 48) sizes,
	which is still retrieved when browsers display HTTP error pages, PDFs, images, directory indexes, etc. -->

	<title>A Vector-Level View of GPT-2 - jeremydolan.net</title>

	<style>
		/*** COLORS ***/
		/* Meets WCAG recs for contrast ratio */
		body { background: #ebebeb }
		body { color: #000000 }
		a { color: #c00000 }
		a:active { color: #ff0000 }

		* {	margin: 0; }

		body {
			font-family: Verdana,Geneva,Helvetica,Arial,sans-serif;
			/* text-align: center; */
			max-width: 980px;
			padding: 0 2em;
			margin: auto;
		}
		header {
			margin-top: 2em;
			margin-bottom: 3em;
			text-align: center;
		}
		h1 {
			font-size: 1.75em;
			font-weight: bold;
			margin-top: 1em;
		}
		p.home-link {
			font-size: 0.8em;
			margin-top: 0;	
		}
		hr { margin: 1.5em; }
		h2 {
			font-size: 1.1em;
			font-weight: bold;
			margin: 1em;
			text-align: center;
		}
		p {
			margin: 0.75em 0;
		}
		p.with {
			margin: 0.75em -20px;
		}
		div.draft {
            text-align: center;
			margin-bottom: 3rem;
        }
		div.view {
            font-size: 1.75rem;
			margin-bottom: 4rem;
			margin-top: 4rem;
		}
		ol.goals p:first-of-type {
			margin-top: 0;
		}
		/* ol.goals li::marker { font-weight: bold; } */
		footer {
            font-size: 0.8em;
            margin-top: 3em;
            margin-bottom: 1em;
            text-align: center;
        }
	</style>
</head>
<body>
	<header>
        <h1>A Vector-Level View of GPT-2</h1>
        <p>Computational infographic of vector-wise inference in a decoder-only transformer<br><br>
           With annotations based on Anthropic’s <a href="https://transformer-circuits.pub/2021/framework/"><i>A
           Mathematical Framework for Transformer Circuits</i></a><br>
           and Neel Nanda’s <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J"><i>Comprehensive Mechanistic Interpretability Explainer &amp; Glossary</i></a>
        </p>
	</header>

    <div class="draft">
		<div class="view"><a href="gpt2.svg">View the graphic</a></div>
		Source on <a href="https://github.com/jeremy-dolan/transformer-view">Github</a></p>
		<a href="https://docs.google.com/document/d/1F_kcOAr5N37XkE2eVdhn8BWnlX2SkCDvf5z63fN_zVU/edit"><em>Feedback welcome!</em></a>
	</div>

	<p>This graphic situates some of the core insights from the mechanistic interpretability research program in a
	visual walkthrough of a transformer model. The goal is to blend...</p>
   
	<ol class="goals">
		<li>
			<b>A <em>vector-level</em> computational graph</b>
			<div>
				<p>Neural networks are typically implemented as a series of <em>tensor operations</em> because modern
				tooling is highly optimized for the parallelized matrix multiplications that tensors facilitate. But the
				most computationally efficient way to <em>code</em> a neural network isn’t necessarily the best way to
				understand how it works. Vectors (aka embeddings) are the fundamental information-bearing units in
				transformers, and are—with few exceptions—operated on completely independently. Discussions
				framed in terms of [batch &times; head &times; position &times; d_head] tensors, where thousands of
				high-dimensional vectors are packed together, can lose focus on how information actually flows through
				the model.</p>

				<p>Implementations sometimes even permute the computational structure of the architecture for
				efficiency. For example, <a href="https://arxiv.org/abs/1706.03762">the original transformers paper</a>
				describes multi-headed attention as involving a concatenation of the result vectors from each head,
				which is then projected back to the residual stream. Implementations and discussion since has largely
				adhered to this structuring. But concatenation is an unprincipled operation that obscures the natural
				way information flows through attention heads: result vectors are independently meaningful, and they can
				be directly and independently projected back to the residual stream without any concatenation
				operation.</p>
			</div>
		</li>

        <p class="with">with,</p>

		<li>
			<b>A mechanistic interpretability infographic</b>
			<div>
				<p>Existing work (such as Anthropic’s excellent <a href="https://transformer-circuits.pub/">Transformer
				Circuits</a> thread) is weighty, and our understanding is rapidly evolving. A good primer might help
				people bootstrap into this important research program.</p>
			</div>
		</li>
	</ol>

	<p>The intended audience already has a rough understanding of the transformer architecture. If a refresher is
	needed, I recommend Jay Alamar’s <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated
	Transformer</a>. Note that my diagram depicts a decoder-only model (GPT-2 124M, a common reference model for
	interpretability work) rather than the original encoder-decoder architecture depicted in Alamar’s piece.</p>

	<p><i>Created for BlueDot Impact’s AI Safety Fundamentals’ <a href="https://aisafetyfundamentals.com/alignment/">AI
	Alignment Course</a>. Thanks to Hannes Whittingham for feedback and encouragement; check out his final project on <a
	href="https://hannewhitt.github.io/llm-aligned-rl/">Reinforcement Learning from LLM Feedback</a>!</i></p>

	<hr>
	<p><b>Sources</b></p>
	<ul>
		<li>Anthropic interpretability team:
			<ul>
				<li>The <a href="https://transformer-circuits.pub/">Transformer Circuits</a> series of papers,
				particularly: <a href="https://transformer-circuits.pub/2021/framework/">A Mathematical Framework for
				Transformer Circuits</a></li>
				<li><a href="https://www.youtube.com/playlist?list=PLoyGOS2WIonajhAVqKUgEMNmeq3nEeM51">Transformer
				Circuits: rough early thoughts</a> (series of research talks)</li>
			</ul>
		</li>
		<li>DeepMind mechanistic interpretability team:
			<ul>
				<li><a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">Fact
				Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level</a></li>
			</ul>
		</li>
		<li>Neel Nanda:
			<ul>
				<li><a href="https://www.youtube.com/watch?v=bOYE6E8JrtU">What is a Transformer?</a> video</li>
				<li><a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J">Comprehensive Mechanistic Interpretability
				Explainer &amp; Glossary</a></li>
			</ul>
		</li>
	</ul>
	<p><b>Recommended introductory resources</b></p>
	<ul>
		<li>Jay Alamar:
			<ul>
				<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
			</ul>
		</li>
		<li>3blue1brown:
			<ul>
				<li>The <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">deep learning</a> video series, particularly:
			<a href="https://www.youtube.com/watch?v=wjZofJX0v4M">Transformers</a>,
			<a href="https://www.youtube.com/watch?v=eMlx5fFNoYc">Attention</a>, and
			<a href="https://www.youtube.com/watch?v=9-Jl0dxWQs8">How might LLMs store facts</a></li>
			</ul>
		</li>
	</ul>

    <footer>
		<hr>
		<a href="/">jeremydolan.net</a>
	</footer>
</body>
</html>
